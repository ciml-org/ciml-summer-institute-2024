{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa97a95-52c5-4e58-81f7-ed62c916ce27",
   "metadata": {},
   "source": [
    "# RAG – Retrieval Augmented Generation\n",
    "\n",
    "## CIML Summer Institute\n",
    "\n",
    "#### UC San Diego\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9d0f8c-b30a-4e71-9e0b-b1f431c5372b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In this RAG tutorial, we'll be working with [LangChain](https://python.langchain.com/v0.2/docs/introduction/), which is a powerful framework for building applications with language models. LangChain provides utilities for working with various language model providers, integrating embeddings, and creating chains for more complex applications. Below are the necessary imports for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0922911-8123-41a2-85b9-f7b83e725776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ca64f-e1ee-4a9d-bf54-57e8c195d04a",
   "metadata": {},
   "source": [
    "We are also using [Ollama](https://ollama.com/), which is a platform for running LLMs on your local machine.  The following steps are needed to set up ollama for this RAG tutorial:\n",
    "1. In a terminal window in JupyterLab, type in the following command to start up the ollama service:\n",
    "**ollama serve**\n",
    "2. In another terminal window in JupyterLab, type in the following to download the model:  **ollama pull mistral**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c28c5e-392e-4ae0-8afe-dd663f32ce83",
   "metadata": {},
   "source": [
    "## Part 1: Retrieval\n",
    "\n",
    "- In this section, we'll focus on the retrieval aspect of RAG. We'll start by understanding vectorization, followed by storing and retrieving vectors efficiently.\n",
    "\n",
    "#### Vectorizing\n",
    "\n",
    "- **Vectorization** is the process of converting text into vectors in an embedding space. These vectors capture the semantic meaning of the text, enabling us to perform various operations like similarity calculations. We'll use HuggingFaceEmbeddings for this task, which is a class in LangChain that allows for an **embedding model** from HuggingFace to be integrated into LangChain applications. You can see the documentation for this LangChain class [here](https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.huggingface.HuggingFaceEmbeddings.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f763639e-a34c-4223-a11c-b838ae29f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258be401-ce04-43e0-9400-8a51bfb69a9f",
   "metadata": {},
   "source": [
    "This vectorizer converts text into vectors in embedding space. Lets try seeing how we can use this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5d355ea-1406-4da7-9778-f11efd6062e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.05314699932932854,\n",
       " 0.014194376766681671,\n",
       " 0.0071457442827522755,\n",
       " 0.06860868632793427,\n",
       " -0.0784803256392479,\n",
       " 0.010167454369366169,\n",
       " 0.10228314995765686,\n",
       " -0.012064827606081963,\n",
       " 0.09521343559026718,\n",
       " -0.030350156128406525]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.embed_query(\"dog\")[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1ed0d1-412b-40dc-9a60-6ac60e88cf3d",
   "metadata": {},
   "source": [
    "- As you can see from above, this converts text into a series of numbers. \n",
    "\n",
    "### Task 1\n",
    "\n",
    "Your job is to **write a function that takes in two strings, vectorize them, and return their cosine similarity.** Implement the following function.\n",
    "\n",
    "#### `similarity_two_queries`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2957aa2c-911d-4a34-9e76-a396384a9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_two_queries(word1, word2):\n",
    "    # HINT:\n",
    "    # Use vectorizer.embed_query(<text>) to embed text.\n",
    "    # Use np.dot to find the cosine similarity/dot product of 2 vectors\n",
    "    # TODO\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61d9536c-aef6-4af4-9c7b-8611d4c92652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def similarity_two_queries(word1, word2):\n",
    "    # TODO\n",
    "    word1_vec = vectorizer.embed_query(word1)\n",
    "    word2_vec = vectorizer.embed_query(word2)\n",
    "    return np.dot(word1_vec,word2_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0521f6f-630e-4e40-b66e-957c8e00f110",
   "metadata": {},
   "source": [
    "- Observe the similarity scores of both **'cat'** and **'dog'** to the word **'kitten'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22c99caa-1b2f-4d53-92f3-140480908aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of 'kitten' and 'cat':  0.7882107945392884\n",
      "Similarity of 'kitten' and 'dog':  0.520505095530218\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity of 'kitten' and 'cat': \",similarity_two_queries(\"kitten\",\"cat\"))\n",
    "print(\"Similarity of 'kitten' and 'dog': \",similarity_two_queries(\"kitten\",\"dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f121272b-fe4a-488c-89ed-320a6ae5dd24",
   "metadata": {},
   "source": [
    "- By using the previously defined function,  we can take pairs of texts and quantify how **similar** they are.\n",
    "\n",
    "### Task 2\n",
    "\n",
    "**Which of the following words in the list `words` are most related to the word 'color'?** The function `similarity_list` takes a list of words, and outputs the word and similarity score from highest to lowest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ec8100d-03b8-4b5e-beee-1959fc5250bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_list(word,list):\n",
    "    similarity_list = [(i,similarity_two_queries(\"color\",i)) for i in words]\n",
    "    sorted_similarity_list = sorted(similarity_list,key=lambda x:x[1],reverse=True)\n",
    "    return sorted_similarity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d23161b1-5eb0-4646-a3e0-0d757873082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"rainbow\",\"car\",\"black\",\"red\",\"cat\",\"tree\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec1d4d3d-e7d0-424e-a040-cb0481f19b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Which words are most similar to color?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b8e6f85-cb59-4739-a0f5-960e9c212315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('black', 0.7855720868733791),\n",
       " ('red', 0.7491880062040888),\n",
       " ('rainbow', 0.5601088091565491),\n",
       " ('car', 0.4040626690053575),\n",
       " ('cat', 0.3583904470705671),\n",
       " ('tree', 0.357355130693161)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "similarity_list(\"color\",words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1408298c-e28c-4876-bc11-4d90510b9bf2",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Each query below has an appropriate text that allows you to answer the question. The function `match_queries_with_texts` matches a query with its most related text. **Come up with 3 more questions and 3 suitable answers and add them to the list below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f6b1718-3887-4923-8710-41f3ab0d64ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_queries_with_texts(queries, texts):\n",
    "    # Calculate similarities between each query and text\n",
    "    similarities = np.zeros((len(queries), len(texts)))\n",
    "    \n",
    "    for i, query in enumerate(queries):\n",
    "        for j, text in enumerate(texts):\n",
    "            similarities[i, j] = similarity_two_queries(query, text)\n",
    "    \n",
    "    # Match each query to the text with the highest similarity\n",
    "    matches = {}\n",
    "    for i, query in enumerate(queries):\n",
    "        best_match_idx = np.argmax(similarities[i])\n",
    "        matches[query] = texts[best_match_idx]\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a62c3cf-808c-4368-b400-e63e95ccb3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the list to make suitable question-text pairs.\n",
    "\n",
    "queries = [\"What are the 7 colors of the rainbow?\", \n",
    "           \"What does Elsie do for work?\", \n",
    "           \"Which country has the largest population?\",\n",
    "           \"-- INSERT QUERY 1 HERE--\",\n",
    "           \"-- INSERT QUERY 2 HERE--\",\n",
    "           \"-- INSERT QUERY 3 HERE--\"]\n",
    "texts = [\"China has 1.4 billion people.\",\n",
    "         \"Elsie works the register at Arby's.\", \n",
    "         \"The colors of the rainbow are ROYGBIV.\",\n",
    "         \"-- INSERT TEXT 1 HERE--\",\n",
    "         \"-- INSERT TEXT 2 HERE--\",\n",
    "         \"-- INSERT TEXT 3 HERE--\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f575526f-49eb-4beb-8ee0-f61bf0ec04c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOLUTION\n",
    "queries = [\"What are the 7 colors of the rainbow?\", \n",
    "           \"What does Elsie do for work?\", \n",
    "           \"Which country has the largest population?\",\n",
    "           \"What time is it?\",\n",
    "           \"What is the largest continent?\",\n",
    "           \"Who is the greatest Football player?\"]\n",
    "texts = [\"China has 1.4 billion people.\",\n",
    "         \"Elsie works the register at Arby's.\", \n",
    "         \"The colors of the rainbow are ROYGBIV.\",\n",
    "         \"The time is 3:14.\",\n",
    "         \"The largest continent is Asia.\",\n",
    "         \"Christiano Ronaldo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc7f7c6-944b-4142-b3ec-546e9fc63920",
   "metadata": {},
   "source": [
    "- Now we shuffle the queries and texts. Let's see if we can match them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7475dd2e-05e4-4ec8-9de4-006edfa606c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Who is the greatest Football player?': 'Christiano Ronaldo',\n",
       " 'What are the 7 colors of the rainbow?': 'The colors of the rainbow are ROYGBIV.',\n",
       " 'What time is it?': 'The time is 3:14.',\n",
       " 'Which country has the largest population?': 'China has 1.4 billion people.',\n",
       " 'What does Elsie do for work?': \"Elsie works the register at Arby's.\",\n",
       " 'What is the largest continent?': 'The largest continent is Asia.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(queries)\n",
    "random.shuffle(texts)\n",
    "\n",
    "match_queries_with_texts(queries, texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24525a35-9b13-46b3-9aa2-8dbe48714dfb",
   "metadata": {},
   "source": [
    "#### Database\n",
    "\n",
    "Now lets look at how we can store these for efficient retrieval of the vectors. There are many options for storage but in this exercise, we use [ChromaDB](https://python.langchain.com/v0.1/docs/integrations/vectorstores/chroma/)\n",
    " which is an open-source vector DB.\n",
    "\n",
    "Through langchain, we can set the database to be a LangChain ***retriever*** object, which essentially allows us to perform queries similarly to what we have done before.\n",
    "\n",
    "**Taking the `texts` and `queries` that you defined before, we can load it into ChromaDB and similarly perform the same operations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ca3bf18-a3c8-421a-9bf1-622372a250b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(range(len(texts)))\n",
    "db = Chroma.from_texts(texts, vectorizer, metadatas=[{\"id\": id} for id in ids])\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59bb640c-8719-4095-9bf5-ceeb7051a5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The time is 3:14.',\n",
       " 'The colors of the rainbow are ROYGBIV.',\n",
       " \"Elsie works the register at Arby's.\",\n",
       " 'The largest continent is Asia.',\n",
       " 'China has 1.4 billion people.',\n",
       " 'Christiano Ronaldo']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "975f0dac-6bfd-4e3b-b6e3-e432122a2dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='China has 1.4 billion people.', metadata={'id': 4})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Which country has the largest population?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdfd53f-3620-4fa6-bc65-6d2466f76ca6",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "Now let’s apply the same retrieval process to a file we read in. The file `workplaces.txt` contains names and workplaces of several people. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca03765b-8e45-4db0-adcc-3b9eb2803f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Aaron works at McDonald's\", 'Beth works at Starbucks', 'Charlie works at Walmart', 'Daisy works at Amazon']\n"
     ]
    }
   ],
   "source": [
    "with open(\"workplaces.txt\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "lines = [line.strip() for line in lines]\n",
    "print(lines[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062cf178-d4c9-4910-845f-13f700a120e2",
   "metadata": {},
   "source": [
    "`workplace_retriever` is a function that takes in the workplace.txt file and returns a database as retriever that you can use to find out the workplaces of people in the file. You can specify the top-k results in the argument of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dead1bb8-2f88-417d-9a22-4de7f9685354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def workplace_retriever(k=3):\n",
    "    with open(\"workplaces.txt\", 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    lines = [line.strip() for line in lines]\n",
    "    db = Chroma.from_texts(lines,vectorizer, metadatas=[{\"id\": id} for id in list(range(len(lines)))])\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": k})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1679c17b-1943-4558-aec0-0a201c84a906",
   "metadata": {},
   "source": [
    "Using `workplace_retriever`, **find out who works at Starbucks and McDonald's**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfd4b386-f9fb-437f-8951-ad8400adcb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find out who works at Starbucks and who works at McDonalds. Use the retriever(k=3).invoke(<query>) method to do this\n",
    "# You can experiment with the value of k to make sure you find all people that work in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "faa40354-ce6a-4313-a108-806402de4f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Brian works at Starbucks', metadata={'id': 27}),\n",
       " Document(page_content='Beth works at Starbucks', metadata={'id': 1}),\n",
       " Document(page_content=\"Aaron works at McDonald's\", metadata={'id': 0})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "workplace_retriever(3).invoke(\"Who works at Starbucks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c42cf44b-85f5-4576-9472-cedc965aa288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Aaron works at McDonald's\", metadata={'id': 0}),\n",
       " Document(page_content=\"Alice works at McDonald's\", metadata={'id': 26}),\n",
       " Document(page_content='Wendy works at Reddit', metadata={'id': 22})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "workplace_retriever(3).invoke(\"Who works at McDonald's\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747bcccb-8dd3-4fc2-ba37-2c10ae4b409d",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "#### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6852173-e73d-4571-a1b8-6d3342a51117",
   "metadata": {},
   "source": [
    "The `workplaces.txt` data we just looked at was conveniently split into lines, with each line representing a distinct and meaningful chunk of information. This straightforward structure makes it easier to process and analyze the text data.\n",
    "\n",
    "However, it is usually not so straightforward:\n",
    "- When dealing with text data, especially from large or complex documents, it's essential to handle the formatting and structure efficiently.\n",
    "- If we get a not-so-simply formatted file, we can break it down into manageable chunks using LangChain's `TextLoader` and `RecursiveCharacterTextSplitter`.\n",
    "- This allows us to preprocess and chunk the data effectively for further use in our RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aa4dd5-660b-4586-b4c0-934944f3233e",
   "metadata": {},
   "source": [
    "Lets take a look at some of the *Expanse* documentation [here](https://www.sdsc.edu/support/user_guides/expanse.html). We have downloaded the contents of this webpage into two text files named `expanse_doc_1.txt` and `expanse_doc_2.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90806506-2a56-4a27-bd74-ae0d44f29abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Job Charging', 'Compiling', 'Running Jobs', 'GPU Nodes', 'Data Movement', 'Storage', 'Composable Systems', 'Software Packages', 'Publications', 'Expanse User Guide', 'Technical Summary', '', '', 'Expanse is a dedicated Advanced Cyberinfrastructure Coordination Ecosystem: Services and Support (ACCESS) cluster designed by Dell and SDSC delivering 5.16 peak petaflops, and will offer Composable Systems and Cloud Bursting.', '']\n"
     ]
    }
   ],
   "source": [
    "with open(\"expanse_doc_1.txt\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "lines = [line.strip() for line in lines]\n",
    "print(lines[20:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9062ca2-37f0-4aee-95fd-9e70dbe8bd45",
   "metadata": {},
   "source": [
    "- We see that the data and text is not split into meaningful chunks of information by default, so we need to try out best to format it in such a way it can be useful. This is why we use chunks, which capture local and neighboring texts, grouping them together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1e0e3a-3093-4fcf-ae68-a2f92c3c3642",
   "metadata": {},
   "source": [
    "A function that chunks `expanse_doc_1.txt` has been provided below.  When using the RecursiveCharacterTextSplitter, the chunk size determines the maximum size of each text chunk. This is particularly useful when dealing with large documents that need to be split into smaller, manageable pieces for better retrieval and analysis.\n",
    "\n",
    "**Experiment with different chunk sizes** and pick a size that captures enough information to answer the question: ***\"How do you run jobs on expanse?\"*** Try sizes **10, 100 and 1000** and observe what info is being given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "516bc647-7988-4aa5-9154-2e72af5367a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanse_retriever(chunk_size):\n",
    "    loader = TextLoader('expanse_doc_1.txt')\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=10, separators=[\" \", \",\", \"\\n\"])\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    db = Chroma(embedding_function=vectorizer)\n",
    "    db.add_documents(texts)\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "251c533f-4fa7-4f60-b7bb-8ea894404e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Think about how many characters would be needed to contain useful information for such a complex task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02367974-88ac-48c0-8fdc-b8c1499b20cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='up to 30M core-hours.\\nJob Scheduling Policies\\nThe maximum allowable job size on Expanse is 4,096 cores – a limit that helps shorten wait times since there are fewer nodes in idle state waiting for large number of nodes to become free.\\nExpanse supports long-running jobs - run times can be extended to one week. Users requests will be evaluated based on number of jobs and job size. \\nExpanse supports shared-node jobs (more than one job on a single node). Many applications are serial or can only scale to a few cores. Allowing shared nodes improves job throughput, provides higher overall system utilization, and allows more users to run on Expanse.\\nTechnical Details\\nSystem Component\\tConfiguration\\nCompute Nodes\\nCPU Type\\tAMD EPYC 7742\\nNodes\\t728\\nSockets\\t2\\nCores/socket\\t64\\nClock speed\\t2.25 GHz\\nFlop speed\\t4608 GFlop/s\\nMemory capacity\\t\\n* 256 GB DDR4 DRAM\\n\\nLocal Storage\\t\\n1TB Intel P4510 NVMe PCIe SSD\\n\\nMax CPU Memory bandwidth\\t409.5 GB/s\\nGPU Nodes\\nGPU Type\\tNVIDIA V100 SMX2\\nNodes\\t52\\nGPUs/node\\t4\\nCPU', metadata={'source': 'expanse_doc_1.txt'}),\n",
       " Document(page_content='Compute Units (SSCUs), comprising 728 standard nodes, 54 GPU nodes and 4 large-memory nodes. Every Expanse node has access to a 12 PB Lustre parallel file system (provided by Aeon Computing) and a 7 PB Ceph Object Store system. Expanse uses the Bright Computing HPC Cluster management system and the SLURM workload manager for job scheduling.\\n\\nExpanse Portal Login\\n\\nExpanse supports the ACCESS core software stack, which includes remote login, remote computation, data movement, science workflow support, and science gateway support toolkits.\\n\\nExpanse is an NSF-funded system operated by the San Diego Supercomputer Center at UC San Diego, and is available through the ACCESS program.\\n\\nResource Allocation Policies\\nThe maximum allocation for a Principle Investigator on Expanse is 15M core-hours and 75K GPU hours. Limiting the allocation size means that Expanse can support more projects, since the average size of each is smaller.\\nScience Gateways requesting in the Maximize tier can request up to', metadata={'source': 'expanse_doc_1.txt'}),\n",
       " Document(page_content=\"script provides additional details regarding project availability and usage.  The script is located at:\\n\\n/cm/shared/apps/sdsc/current/bin/expanse-client\\n\\nThe script uses the 'sdsc' module, which is loaded by default. \\n\\n[user@login01 ~]$ module load sdsc\\n \\nTo review your available projects on Expanse resource use the 'user' parameter and '-r' to desginate a resource.  If no resouce is designated expanse data will be shown by default.\\n\\nuser@login01 ~]$ expanse-client user -r expanse\\n\\nResource expanse\\n\\n╭───┬─────────────┬─────────┬────────────┬──────┬───────────┬─────────────────╮\\n│   │ NAME        │ PROJECT │ TG PROJECT │ USED │ AVAILABLE │ USED BY PROJECT │\\n├───┼─────────────┼─────────┼────────────┼──────┼───────────┼─────────────────┤\\n│ 1 │ user        │ ddp386  │            │ 0    │ 110000    │ 8318            │\\n╰───┴─────────────┴─────────┴────────────┴──────┴───────────┴─────────────────╯\\n\\nTo see full list of available resources, use the 'resource' command:\\n\\n[user@login02 ~]$\", metadata={'source': 'expanse_doc_1.txt'})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "expanse_retriever(1000).invoke(\"How do you run jobs on expanse?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a48cb82-d6a2-4785-a68d-12437236633c",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "#### Multiple Document Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68743bad-9b2d-44c1-9cb0-1f4203869467",
   "metadata": {},
   "source": [
    "When we have more than one document we want to use in our database, we can simply iteratively chunk them. Metadata for the text source is added by default, but we can add our own metadata as well in the form of IDs.\n",
    "\n",
    "\n",
    "`expanse_all_retriever` is a function that chunks both `expanse_doc_1.txt` and `expanse_doc_2.txt` has been provided below, using a chunk size of 1000 characters, **Find which document information for \"Compiling Codes\" is most likely to be located.** *Hint: Look at the metadata*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d746a3dd-e674-41c3-b0f8-22856530f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanse_all_retriever(chunk_size):\n",
    "    import glob\n",
    "    db = Chroma(embedding_function=vectorizer)\n",
    "    pattern = 'expanse_doc_*.txt'\n",
    "    file_list = glob.glob(pattern)\n",
    "    for file_name in file_list:\n",
    "        loader = TextLoader(file_name)\n",
    "        documents = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=10, separators=[\" \", \",\", \"\\n\"])\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        for id,text in enumerate(texts):\n",
    "            text.metadata[\"chunk_number\"] = id\n",
    "        db.add_documents(texts)\n",
    "\n",
    "    \n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "642fd129-85b9-4909-92e4-212cee3345ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find the relevant source for the query \"Compiling Codes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdf79abe-d9ca-4569-bea4-f21440df1e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'expanse_doc_2.txt', 'chunk_number': 0}\n",
      "{'source': 'expanse_doc_2.txt', 'chunk_number': 1}\n",
      "{'source': 'expanse_doc_2.txt', 'chunk_number': 5}\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "chunks = expanse_all_retriever(1000).invoke(\"Compiling Codes\")\n",
    "for chunk in chunks:\n",
    "    print(chunk.metadata)\n",
    "\n",
    "# The answer is expanse_doc_2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f092f17f-faeb-4fee-8a00-3697a71988fa",
   "metadata": {},
   "source": [
    "## Part 2: Basic RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66182b0-291a-4913-803c-1f2894af1310",
   "metadata": {},
   "source": [
    "Ollama is an open-source LLM platform that allows us to use a plethora of different LLMs. In this notebook, Mistral is our LLM of choice. Feel free to play around with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "114e1036-dfc4-4ebf-abb8-b2da98b03e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I'm just a computer program, so I don't have feelings or experiences like humans do. But I'm here and ready to help you with any question you might have! Let's chat! What can I assist you with today?\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama = Ollama(model=\"mistral\")\n",
    "ollama.invoke(\"How are you doing?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fae322-2770-45c2-a3d9-245f6ecaae55",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "\n",
    "**Write a function that uses the `workplace_retriever` function to parse your question, retrieves relevant responses from `workplace_retriever`, and then sends this context to Ollama for it to answer your question in natural language.** Fill in `workplace_question` which accomplishes this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e9fd5c4-32f0-40be-8823-90924dcfab5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1314514365.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[31], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    retriever = #TODO: assign the retriever\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "def workplace_question(question):\n",
    "    retriever = #TODO: assign the retriever\n",
    "    context = #TODO: invoke the retriever here\n",
    "    ollama = Ollama(model=\"mistral\")\n",
    "    prompt = f\"Based on the following context: {context}, answer the question: \"\n",
    "    response = #TODO: invoke ollama with the prompt and question\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88b53c73-8369-4e89-8190-9862ebc93483",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOLUTION\n",
    "def workplace_question(question):\n",
    "    retriever = workplace_retriever()\n",
    "    context = retriever.invoke(question)\n",
    "    ollama = Ollama(model=\"mistral\")\n",
    "    prompt = f\"Based on the following context: {context}, answer the question: \"\n",
    "    response = ollama.invoke(prompt + question)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c70a7fba-ef08-41cf-861b-255f76fcea28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The individuals who work at Starbucks are Brian (from document with id 27) and Beth (from document with id 1).'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workplace_question(\"Who works at Starbucks?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f0ca36-fcf4-449d-a21e-3e9310e39eaa",
   "metadata": {},
   "source": [
    "## Part 3: LangChain RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fda8a1-2152-4d38-bbee-775ba0d49110",
   "metadata": {},
   "source": [
    "The above is a very simple example of a RAG. Now, using langchain, we can put everything together in a cleaner and all inclusive way in one go. Let's combine everything we've learned into the function `generate_rag`.\n",
    "\n",
    "- The below implementation has a custom class that allows us to view what chunks are being used based on our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bedb5d18-0871-490b-b7d9-42c582918150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rag(verbose=False, chunk_info=False):\n",
    "    import glob\n",
    "    vectorizer = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    db = Chroma(embedding_function=vectorizer)\n",
    "    pattern = 'expanse_doc_*.txt'\n",
    "    file_list = glob.glob(pattern)\n",
    "    for file_name in file_list:\n",
    "        loader = TextLoader(file_name)\n",
    "        documents = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=10, separators=[\" \", \",\", \"\\n\"])\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "        for id,text in enumerate(texts):\n",
    "            text.metadata[\"chunk_number\"] = id\n",
    "        db.add_documents(texts)\n",
    "    \n",
    "    template = \"\"\"<s>[INST] Given the context - {context} </s>[INST] [INST] Answer the following question - {question}[/INST]\"\"\"\n",
    "    pt = PromptTemplate(\n",
    "                template=template, input_variables=[\"context\", \"question\"]\n",
    "            )\n",
    "    # Let's retrieve the top 3 chunks for our results\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "    class CustomRetrievalQA(RetrievalQA):\n",
    "        def invoke(self, *args, **kwargs):\n",
    "            result = super().invoke(*args, **kwargs)\n",
    "            if chunk_info:\n",
    "                # Print out the chunks that were retrieved\n",
    "                print(\"Chunks being looked at:\")\n",
    "                chunks = retriever.invoke(*args, **kwargs)\n",
    "                for chunk in chunks:\n",
    "                    print(f\"Source: {chunk.metadata['source']}, Chunk number: {chunk.metadata['chunk_number']}\")\n",
    "                    print(f\"Text snippet: {chunk.page_content[:200]}...\\n\")  # Print the first 200 characters\n",
    "            return result\n",
    "    rag = CustomRetrievalQA.from_chain_type(\n",
    "        llm=Ollama(model=\"mistral\"),\n",
    "        retriever=retriever,\n",
    "        memory=ConversationSummaryMemory(llm=Ollama(model=\"mistral\")),\n",
    "        chain_type_kwargs={\"prompt\": pt, \"verbose\": verbose},\n",
    "    )\n",
    "\n",
    "    return rag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f85b1e-4d5e-46b7-9fc4-82a993b2af2d",
   "metadata": {},
   "source": [
    "### Task 8\n",
    "**Compare how mistral performs without context, and with context, i.e. without RAG and with RAG.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27716e1a-d3e7-4a3d-aa75-27a29919d06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To check the available resources on an Expanse Supercomputer, you would typically use command-line tools or graphical interfaces provided by the supercomputing system. Here's a general guide for using some common commands:\n",
      "\n",
      "1. **SSH (Secure Shell)**: You can connect to your allocated node(s) using SSH and then run commands to check resource availability. For example, you might use `uname -a` to get detailed system information, or `free -h` to see memory usage, among other commands.\n",
      "\n",
      "2. **Resource Management System (RMS)**: Most supercomputers have a Resource Management System like Slurm, LSF, or Torque. These systems allow you to submit jobs for processing and monitor their status. To check resource availability in such a system, you can use commands like `squeue` to see currently running jobs, pending jobs, and resource usage.\n",
      "\n",
      "3. **Monitoring Tools**: Some supercomputers have monitoring tools that provide real-time or historical data about the system's performance and resource utilization. Examples include Ganglia, Nagios, or OpenNebula. The exact commands to use will depend on the specific monitoring tool being used.\n",
      "\n",
      "Before running any of these commands, make sure you have the necessary permissions and understand the potential impact of your actions. It's always a good idea to consult with your system administrator if you're unsure about anything.\n"
     ]
    }
   ],
   "source": [
    "print(ollama.invoke(\"How do you check available resources on Expanse Supercomputer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1dbe7824-f704-4dac-ba1a-55b872a66e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To check the available resources on the Expanse Supercomputer, you can use the command `expanse-client user -r expanse`. This command will display a table showing the list of projects and their usage details on the designated resource (Expanse by default if no resource is specified). If you want to see the full list of available resources, you can use the 'resource' command without any parameters.\n",
      "\n",
      "Here's the command again for your convenience:\n",
      "\n",
      "* To check available projects on Expanse: `[user@login01 ~]$ expanse-client user -r expanse`\n",
      "* To see full list of available resources: `[user@login02 ~]$ resource`\n"
     ]
    }
   ],
   "source": [
    "expanse_rag = generate_rag()\n",
    "result = expanse_rag.invoke(\"How do you check available resources on Expanse Supercomputer\")\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c44d0c4-a5fa-46c5-84a2-16ecc83a42c2",
   "metadata": {},
   "source": [
    "**We can see what is exactly being passed into the LLM highlighted in green when we set `verbose` to True.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2cba2b09-53b6-43d2-a29b-a5c260d87ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] Given the context - script provides additional details regarding project availability and usage.  The script is located at:\n",
      "\n",
      "/cm/shared/apps/sdsc/current/bin/expanse-client\n",
      "\n",
      "The script uses the 'sdsc' module, which is loaded by default. \n",
      "\n",
      "[user@login01 ~]$ module load sdsc\n",
      " \n",
      "To review your available projects on Expanse resource use the 'user' parameter and '-r' to desginate a resource.  If no resouce is designated expanse data will be shown by default.\n",
      "\n",
      "user@login01 ~]$ expanse-client user -r expanse\n",
      "\n",
      "Resource expanse\n",
      "\n",
      "╭───┬─────────────┬─────────┬────────────┬──────┬───────────┬─────────────────╮\n",
      "│   │ NAME        │ PROJECT │ TG PROJECT │ USED │ AVAILABLE │ USED BY PROJECT │\n",
      "├───┼─────────────┼─────────┼────────────┼──────┼───────────┼─────────────────┤\n",
      "│ 1 │ user        │ ddp386  │            │ 0    │ 110000    │ 8318            │\n",
      "╰───┴─────────────┴─────────┴────────────┴──────┴───────────┴─────────────────╯\n",
      "\n",
      "To see full list of available resources, use the 'resource' command:\n",
      "\n",
      "[user@login02 ~]$\n",
      "\n",
      "Compute Units (SSCUs), comprising 728 standard nodes, 54 GPU nodes and 4 large-memory nodes. Every Expanse node has access to a 12 PB Lustre parallel file system (provided by Aeon Computing) and a 7 PB Ceph Object Store system. Expanse uses the Bright Computing HPC Cluster management system and the SLURM workload manager for job scheduling.\n",
      "\n",
      "Expanse Portal Login\n",
      "\n",
      "Expanse supports the ACCESS core software stack, which includes remote login, remote computation, data movement, science workflow support, and science gateway support toolkits.\n",
      "\n",
      "Expanse is an NSF-funded system operated by the San Diego Supercomputer Center at UC San Diego, and is available through the ACCESS program.\n",
      "\n",
      "Resource Allocation Policies\n",
      "The maximum allocation for a Principle Investigator on Expanse is 15M core-hours and 75K GPU hours. Limiting the allocation size means that Expanse can support more projects, since the average size of each is smaller.\n",
      "Science Gateways requesting in the Maximize tier can request up to\n",
      "\n",
      "up to 30M core-hours.\n",
      "Job Scheduling Policies\n",
      "The maximum allowable job size on Expanse is 4,096 cores – a limit that helps shorten wait times since there are fewer nodes in idle state waiting for large number of nodes to become free.\n",
      "Expanse supports long-running jobs - run times can be extended to one week. Users requests will be evaluated based on number of jobs and job size. \n",
      "Expanse supports shared-node jobs (more than one job on a single node). Many applications are serial or can only scale to a few cores. Allowing shared nodes improves job throughput, provides higher overall system utilization, and allows more users to run on Expanse.\n",
      "Technical Details\n",
      "System Component\tConfiguration\n",
      "Compute Nodes\n",
      "CPU Type\tAMD EPYC 7742\n",
      "Nodes\t728\n",
      "Sockets\t2\n",
      "Cores/socket\t64\n",
      "Clock speed\t2.25 GHz\n",
      "Flop speed\t4608 GFlop/s\n",
      "Memory capacity\t\n",
      "* 256 GB DDR4 DRAM\n",
      "\n",
      "Local Storage\t\n",
      "1TB Intel P4510 NVMe PCIe SSD\n",
      "\n",
      "Max CPU Memory bandwidth\t409.5 GB/s\n",
      "GPU Nodes\n",
      "GPU Type\tNVIDIA V100 SMX2\n",
      "Nodes\t52\n",
      "GPUs/node\t4\n",
      "CPU </s>[INST] [INST] Answer the following question - How do you check available resources on Expanse Supercomputer[/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " To check the available resources on Expanse Supercomputer, use the command:\n",
      "\n",
      "[user@login01 ~]$ expanse-client resource\n",
      "\n",
      "This command will display a table with information about each available resource in Expanse. The columns include NAME (name of the resource), PROJECT (project associated with the resource), TG PROJECT (tagged project, if any), USED (currently used resources by the resource), AVAILABLE (the remaining resources for the resource), and USED BY PROJECT (the projects that are currently using the resource). By default, the command will show data from the Expanse resource, but you can specify a different resource with the '-r' option, like this:\n",
      "\n",
      "[user@login01 ~]$ expanse-client user -r <resource_name>\n",
      "\n",
      "For instance, if you want to see resources available in the 'expanse' data resource, use:\n",
      "\n",
      "[user@login01 ~]$ expanse-client user -r expanse\n"
     ]
    }
   ],
   "source": [
    "expanse_rag = generate_rag(verbose=True)\n",
    "result = expanse_rag.invoke(\"How do you check available resources on Expanse Supercomputer\")\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bdc157-a204-4a4b-9970-92aa1b62e239",
   "metadata": {},
   "source": [
    "**For more concise information, the function defined allows us to see individual chunk details as well as their source.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7359d4a-2923-47c3-b0b3-b245681137be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks being looked at:\n",
      "Source: expanse_doc_1.txt, Chunk number: 12\n",
      "Text snippet: script provides additional details regarding project availability and usage.  The script is located at:\n",
      "\n",
      "/cm/shared/apps/sdsc/current/bin/expanse-client\n",
      "\n",
      "The script uses the 'sdsc' module, which is lo...\n",
      "\n",
      "Source: expanse_doc_1.txt, Chunk number: 1\n",
      "Text snippet: Compute Units (SSCUs), comprising 728 standard nodes, 54 GPU nodes and 4 large-memory nodes. Every Expanse node has access to a 12 PB Lustre parallel file system (provided by Aeon Computing) and a 7 P...\n",
      "\n",
      "Source: expanse_doc_1.txt, Chunk number: 2\n",
      "Text snippet: up to 30M core-hours.\n",
      "Job Scheduling Policies\n",
      "The maximum allowable job size on Expanse is 4,096 cores – a limit that helps shorten wait times since there are fewer nodes in idle state waiting for lar...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expanse_rag = generate_rag(chunk_info=True)\n",
    "result = expanse_rag.invoke(\"How do you check available resources on Expanse Supercomputer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2b21255-05d4-47d1-a1a7-5624f24e7a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To check the available resources on the Expanse Supercomputer, you can use the command `expanse-client user -r expanse`. This will display a table of available projects on the Expanse resource.\n",
      "\n",
      "Here's how to execute it:\n",
      "\n",
      "```bash\n",
      "[user@login01 ~]$ module load sdsc\n",
      "[user@login01 ~]$ expanse-client user -r expanse\n",
      "```\n",
      "\n",
      "If you want to see a full list of available resources, use the `resource` command without any parameters:\n",
      "\n",
      "```bash\n",
      "[user@login02 ~]$ resource\n",
      "```\n",
      "\n",
      "For more detailed information about Expanse resources such as CPU and GPU types, memory capacity, and storage configurations, you can refer to the technical details provided in the context.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc228f4-d6b8-427d-b49b-78214fa606ec",
   "metadata": {},
   "source": [
    "#### Great work! We've officially made a chatbot that can help us out with all things *Expanse*, at least according to the 2 .txt files we have access to!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f1d699-f672-4f64-8261-78cbcc58dd16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
